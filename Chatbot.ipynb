{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMynPP7UhpWRn3JSjSIY7pq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NiklasNesseler/Chatbot/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hPSJIkJOOUt",
        "outputId": "12c77400-178f-47c1-9bf2-95ff8c84d497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_name = \"movie-corpus\"\n",
        "corpus = os.path.join(\"/content/drive/My Drive/\", corpus_name)\n",
        "\n",
        "def printLines(file, n=10):\n",
        "    with open(file, 'rb') as datafile:\n",
        "        lines = datafile.readlines()\n",
        "    for line in lines[:n]:\n",
        "        print(line)\n",
        "\n",
        "printLines(os.path.join(corpus, \"utterances.jsonl\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v94_6fdFSY9-",
        "outputId": "c6324960-fc5f-4bc4-bd09-e7469aa99187"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'{\"id\": \"L1045\", \"conversation_id\": \"L1044\", \"text\": \"They do not!\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"not\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L1044\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L1044\", \"conversation_id\": \"L1044\", \"text\": \"They do to!\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"They\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"dobj\", \"up\": 1, \"dn\": []}, {\"tok\": \"!\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L985\", \"conversation_id\": \"L984\", \"text\": \"I hope so.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"hope\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 2, 3]}, {\"tok\": \"so\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 1, \"dn\": []}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": \"L984\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L984\", \"conversation_id\": \"L984\", \"text\": \"She okay?\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 1, \"toks\": [{\"tok\": \"She\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"okay\", \"tag\": \"RB\", \"dep\": \"ROOT\", \"dn\": [0, 2]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L925\", \"conversation_id\": \"L924\", \"text\": \"Let\\'s go.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Let\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [2, 3]}, {\"tok\": \"\\'s\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"go\", \"tag\": \"VB\", \"dep\": \"ccomp\", \"up\": 0, \"dn\": [1]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L924\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L924\", \"conversation_id\": \"L924\", \"text\": \"Wow\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Wow\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L872\", \"conversation_id\": \"L870\", \"text\": \"Okay -- you\\'re gonna need to learn how to lie.\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 4, \"toks\": [{\"tok\": \"Okay\", \"tag\": \"UH\", \"dep\": \"intj\", \"up\": 4, \"dn\": []}, {\"tok\": \"--\", \"tag\": \":\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"\\'re\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"gon\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 6, 12]}, {\"tok\": \"na\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 6, \"dn\": []}, {\"tok\": \"need\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 8]}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 8, \"dn\": []}, {\"tok\": \"learn\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 6, \"dn\": [7, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 11, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 11, \"dn\": []}, {\"tok\": \"lie\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 8, \"dn\": [9, 10]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": \"L871\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L871\", \"conversation_id\": \"L870\", \"text\": \"No\", \"speaker\": \"u2\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"No\", \"tag\": \"UH\", \"dep\": \"ROOT\", \"dn\": []}]}]}, \"reply-to\": \"L870\", \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L870\", \"conversation_id\": \"L870\", \"text\": \"I\\'m kidding.  You know how sometimes you just become this \\\\\"persona\\\\\"?  And you don\\'t know how to quit?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 2, \"toks\": [{\"tok\": \"I\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 2, \"dn\": []}, {\"tok\": \"\\'m\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 2, \"dn\": []}, {\"tok\": \"kidding\", \"tag\": \"VBG\", \"dep\": \"ROOT\", \"dn\": [0, 1, 3]}, {\"tok\": \".\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 2, \"dn\": [4]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 3, \"dn\": []}]}, {\"rt\": 1, \"toks\": [{\"tok\": \"You\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 1, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VBP\", \"dep\": \"ROOT\", \"dn\": [0, 6, 11]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 3, \"dn\": []}, {\"tok\": \"sometimes\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": [2]}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 6, \"dn\": []}, {\"tok\": \"just\", \"tag\": \"RB\", \"dep\": \"advmod\", \"up\": 6, \"dn\": []}, {\"tok\": \"become\", \"tag\": \"VBP\", \"dep\": \"ccomp\", \"up\": 1, \"dn\": [3, 4, 5, 9]}, {\"tok\": \"this\", \"tag\": \"DT\", \"dep\": \"det\", \"up\": 9, \"dn\": []}, {\"tok\": \"\\\\\"\", \"tag\": \"``\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"persona\", \"tag\": \"NN\", \"dep\": \"attr\", \"up\": 6, \"dn\": [7, 8, 10]}, {\"tok\": \"\\\\\"\", \"tag\": \"\\'\\'\", \"dep\": \"punct\", \"up\": 9, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 1, \"dn\": [12]}, {\"tok\": \" \", \"tag\": \"_SP\", \"dep\": \"\", \"up\": 11, \"dn\": []}]}, {\"rt\": 4, \"toks\": [{\"tok\": \"And\", \"tag\": \"CC\", \"dep\": \"cc\", \"up\": 4, \"dn\": []}, {\"tok\": \"you\", \"tag\": \"PRP\", \"dep\": \"nsubj\", \"up\": 4, \"dn\": []}, {\"tok\": \"do\", \"tag\": \"VBP\", \"dep\": \"aux\", \"up\": 4, \"dn\": []}, {\"tok\": \"n\\'t\", \"tag\": \"RB\", \"dep\": \"neg\", \"up\": 4, \"dn\": []}, {\"tok\": \"know\", \"tag\": \"VB\", \"dep\": \"ROOT\", \"dn\": [0, 1, 2, 3, 7, 8]}, {\"tok\": \"how\", \"tag\": \"WRB\", \"dep\": \"advmod\", \"up\": 7, \"dn\": []}, {\"tok\": \"to\", \"tag\": \"TO\", \"dep\": \"aux\", \"up\": 7, \"dn\": []}, {\"tok\": \"quit\", \"tag\": \"VB\", \"dep\": \"xcomp\", \"up\": 4, \"dn\": [5, 6]}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 4, \"dn\": []}]}]}, \"reply-to\": null, \"timestamp\": null, \"vectors\": []}\\n'\n",
            "b'{\"id\": \"L869\", \"conversation_id\": \"L866\", \"text\": \"Like my fear of wearing pastels?\", \"speaker\": \"u0\", \"meta\": {\"movie_id\": \"m0\", \"parsed\": [{\"rt\": 0, \"toks\": [{\"tok\": \"Like\", \"tag\": \"IN\", \"dep\": \"ROOT\", \"dn\": [2, 6]}, {\"tok\": \"my\", \"tag\": \"PRP$\", \"dep\": \"poss\", \"up\": 2, \"dn\": []}, {\"tok\": \"fear\", \"tag\": \"NN\", \"dep\": \"pobj\", \"up\": 0, \"dn\": [1, 3]}, {\"tok\": \"of\", \"tag\": \"IN\", \"dep\": \"prep\", \"up\": 2, \"dn\": [4]}, {\"tok\": \"wearing\", \"tag\": \"VBG\", \"dep\": \"pcomp\", \"up\": 3, \"dn\": [5]}, {\"tok\": \"pastels\", \"tag\": \"NNS\", \"dep\": \"dobj\", \"up\": 4, \"dn\": []}, {\"tok\": \"?\", \"tag\": \".\", \"dep\": \"punct\", \"up\": 0, \"dn\": []}]}]}, \"reply-to\": \"L868\", \"timestamp\": null, \"vectors\": []}\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loadLinesAndConversations(fileName):\n",
        "  lines = {}\n",
        "  conversations = {}\n",
        "  with open(fileName, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "      lineJson = json.loads(line)\n",
        "      lineObj = {}\n",
        "      lineObj[\"LineID\"] = lineJson[\"id\"]\n",
        "      lineObj[\"characterID\"] = lineJson[\"speaker\"]\n",
        "      lineObj[\"text\"] = lineJson[\"text\"]\n",
        "      lines[lineObj[\"LineID\"]] = lineObj\n",
        "\n",
        "      if lineJson[\"conversation_id\"] not in conversations:\n",
        "        convObj = {}\n",
        "        convObj[\"conversationID\"] = lineJson[\"conversation_id\"]\n",
        "        convObj[\"movieID\"] = lineJson[\"meta\"][\"movie_id\"]\n",
        "        convObj[\"lines\"] = [lineObj]\n",
        "      else:\n",
        "          convObj= conversations[lineJson[\"conversation_id\"]]\n",
        "          convObj[\"lines\"].insert(0, lineObj)\n",
        "      conversations[convObj[\"conversationID\"]] = convObj\n",
        "    return lines, conversations\n",
        "\n",
        "def extractSentencePairs(conversations):\n",
        "    qa_pairs = []\n",
        "    for conversation in conversations.values():\n",
        "      for i in range(len(conversation[\"lines\"]) - 1):\n",
        "        inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
        "        targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "        if inputLine and targetLine:\n",
        "          qa_pairs.append([inputLine, targetLine])\n",
        "    return qa_pairs\n",
        "\n"
      ],
      "metadata": {
        "id": "yjM7UtvnWkFg"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
        "\n",
        "delimiter = '\\t'\n",
        "\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "\n",
        "lines = {}\n",
        "conversations = {}\n",
        "\n",
        "print(\"\\nProcessing corpus into lines and conversations...\")\n",
        "lines, conversations = loadLinesAndConversations(os.path.join(corpus, \"utterances.jsonl\"))\n",
        "\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
        "    for pair in extractSentencePairs(conversations):\n",
        "        writer.writerow(pair)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M1V-vJSYb71",
        "outputId": "18bb001e-99b0-4d17-ab22-a7dabc6ecb6a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing corpus into lines and conversations...\n",
            "\n",
            "Writing newly formatted file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSample lines from file:\")\n",
        "printLines(datafile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqNhE1bSaVTf",
        "outputId": "a9a3c0d1-4f1e-4759-c71f-586183d409f8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample lines from file:\n",
            "b'They do to!\\tThey do not!\\n'\n",
            "b'She okay?\\tI hope so.\\n'\n",
            "b\"Wow\\tLet's go.\\n\"\n",
            "b'\"I\\'m kidding.  You know how sometimes you just become this \"\"persona\"\"?  And you don\\'t know how to quit?\"\\tNo\\n'\n",
            "b\"No\\tOkay -- you're gonna need to learn how to lie.\\n\"\n",
            "b\"I figured you'd get to the good stuff eventually.\\tWhat good stuff?\\n\"\n",
            "b'What good stuff?\\t\"The \"\"real you\"\".\"\\n'\n",
            "b'\"The \"\"real you\"\".\"\\tLike my fear of wearing pastels?\\n'\n",
            "b'do you listen to this crap?\\tWhat crap?\\n'\n",
            "b\"What crap?\\tMe.  This endless ...blonde babble. I'm like, boring myself.\\n\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_token = 0\n",
        "SOS_token = 1\n",
        "EOS_token = 2\n",
        "\n",
        "class Voc:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.trimmed = False\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "    self.num_words = 3\n",
        "\n",
        "  def addSentence(self, sentence):\n",
        "    for word in sentence.split(\" \"):\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.num_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.num_words] = word\n",
        "      self.num_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1\n",
        "\n",
        "  def trim(self, min_count):\n",
        "    if self.trimmed:\n",
        "      return\n",
        "    self.trimmed = True\n",
        "\n",
        "    keep_words = []\n",
        "\n",
        "    for k, v in self.word2count.items():\n",
        "      if v >= min_count:\n",
        "        keep_words.append(k)\n",
        "\n",
        "    print(\"keep_words {} / {} = {:.4f}\".format(len(keep_words),\n",
        "                                               len(self.word2index),\n",
        "                                               len(keep_words) / len(self.word2index)))\n",
        "\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "    self.num_words = 3\n",
        "\n",
        "    for word in keep_words:\n",
        "      self.addWord(word)"
      ],
      "metadata": {
        "id": "tClIRN6maexT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "  return \"\".join(\n",
        "      c for c in unicodedata.normalize(\"NFD\", s)\n",
        "      if unicodedata.category(c) != \"Mn\"\n",
        "  )\n",
        "\n",
        "def normalizeString(s):\n",
        "  s = unicodeToAscii(s.lower().strip())\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "  s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "  return s\n",
        "\n",
        "def readVocs(datafile, corpus_name):\n",
        "  print(\"Reading lines...\")\n",
        "  lines = open(datafile, encoding=\"utf-8\").read().strip().split(\"\\n\")\n",
        "  pairs = [[normalizeString(s) for s in l.split(\"\\t\")] for l in lines]\n",
        "  voc = Voc(corpus_name)\n",
        "  return voc, pairs\n",
        "\n",
        "def filterPair(p):\n",
        "  return len(p[0].split(\" \")) < MAX_LENGTH and len(p[1].split(\" \")) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "  return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def loadPreparedData(corpus, corpus_name, datafile, save_dir):\n",
        "  print(\"Start preparing training data...\")\n",
        "  voc, pairs = readVocs(datafile, corpus_name)\n",
        "  print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
        "  pairs = filterPairs(pairs)\n",
        "  print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "  print(\"Counting words...\")\n",
        "  for pair in pairs:\n",
        "    voc.addSentence(pair[0])\n",
        "    voc.addSentence(pair[1])\n",
        "  print(\"Counted words:\", voc.num_words)\n",
        "  return voc, pairs"
      ],
      "metadata": {
        "id": "49yHTWbOeEiG"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = '/content/drive/MyDrive/chatbot_model'\n",
        "voc, pairs = loadPreparedData(corpus, corpus_name, datafile, save_dir)\n",
        "# Print some pairs to validate\n",
        "print(\"\\npairs:\")\n",
        "for pair in pairs[:10]:\n",
        "  print(pair)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQlj8TCkgXjd",
        "outputId": "3b4d918f-7184-4111-8f55-29735ce6b1e7"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start preparing training data...\n",
            "Reading lines...\n",
            "Read 221282 sentence pairs\n",
            "Trimmed to 64313 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 18082\n",
            "\n",
            "pairs:\n",
            "['they do to !', 'they do not !']\n",
            "['she okay ?', 'i hope so .']\n",
            "['wow', 'let s go .']\n",
            "['what good stuff ?', 'the real you .']\n",
            "['the real you .', 'like my fear of wearing pastels ?']\n",
            "['do you listen to this crap ?', 'what crap ?']\n",
            "['well no . . .', 'then that s all you had to say .']\n",
            "['then that s all you had to say .', 'but']\n",
            "['but', 'you always been this selfish ?']\n",
            "['have fun tonight ?', 'tons']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_COUNT = 3\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "  voc.trim(MIN_COUNT)\n",
        "  keep_pairs = []\n",
        "  for pair in pairs:\n",
        "    input_sentence = pair[0]\n",
        "    output_sentence = pair[1]\n",
        "    keep_input = True\n",
        "    keep_output = True\n",
        "    for word in input_sentence.split(\" \"):\n",
        "      if word not in voc.word2index:\n",
        "        keep_input = False\n",
        "        break\n",
        "    for word in output_sentence.split(\" \"):\n",
        "      if word not in voc.word2index:\n",
        "        keep_output = False\n",
        "        break\n",
        "    if keep_input and keep_output:\n",
        "      keep_pairs.append(pair)\n",
        "  print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs),\n",
        "                                                              len(keep_pairs),\n",
        "                                                              len(keep_pairs) / len(pairs)))\n",
        "  return keep_pairs\n",
        "\n",
        "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEvcT5Kogyqy",
        "outputId": "338d7cc9-1df3-4af6-a03c-b73608e047c2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keep_words 7833 / 18079 = 0.4333\n",
            "Trimmed from 64313 pairs to 53131, 0.8261 of total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(voc, sentence):\n",
        "  return [voc.word2index[word] for word in sentence.split(\" \")] + [EOS_token]\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "print(\"input_variable:\", input_variable)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfdcmeSj6hGa",
        "outputId": "e10845af-bd40-4fb3-fc0d-e10b879f835c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variable: tensor([[ 175,    4,   14,   67, 6571],\n",
            "        [ 180,   24,   14,  176,   10],\n",
            "        [  28,   44, 4769, 1046,    2],\n",
            "        [ 461,    5,   14,  186,    0],\n",
            "        [  24,    4,   14,  587,    0],\n",
            "        [ 512,   36,   14,   10,    0],\n",
            "        [1361,   10,    2,    2,    0],\n",
            "        [  59,    2,    0,    0,    0],\n",
            "        [  10,    0,    0,    0,    0],\n",
            "        [   2,    0,    0,    0,    0]])\n",
            "lengths: tensor([10,  8,  7,  7,  3])\n",
            "target_variable: tensor([[ 185,   44,  208,   99,  280],\n",
            "        [  14,    5,  135,   22,   14],\n",
            "        [   2,   10, 2998, 1065,    2],\n",
            "        [   0,   34,   14,   14,    0],\n",
            "        [   0,   14,  208,    2,    0],\n",
            "        [   0,   14,  135,    0,    0],\n",
            "        [   0,   14,    4,    0,    0],\n",
            "        [   0,    2,   85,    0,    0],\n",
            "        [   0,    0,   14,    0,    0],\n",
            "        [   0,    0,    2,    0,    0]])\n",
            "mask: tensor([[ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [False,  True,  True,  True, False],\n",
            "        [False,  True,  True,  True, False],\n",
            "        [False,  True,  True, False, False],\n",
            "        [False,  True,  True, False, False],\n",
            "        [False,  True,  True, False, False],\n",
            "        [False, False,  True, False, False],\n",
            "        [False, False,  True, False, False]])\n",
            "max_target_len: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = embedding\n",
        "\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "  def forward(self, input_seq, input_lengths, hidden=None):\n",
        "    embedded = self.embedding(input_seq)\n",
        "    packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "    outputs, hidden = self.gru(packed, hidden)\n",
        "    outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "    outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
        "    return outputs, hidden"
      ],
      "metadata": {
        "id": "YXfAaINo-plx"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "      super(Attn, self).__init__()\n",
        "      self.method = method\n",
        "      if self.method not in [\"dot\", \"general\", \"concat\"]:\n",
        "        raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "      self.hidden_size = hidden_size\n",
        "      if self.method == \"general\":\n",
        "        self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "      elif self.method == \"concat\":\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "      return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "      energy = self.attn(encoder_output)\n",
        "      return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "      energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "      return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "      if self.method == \"general\":\n",
        "        attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "      elif self.method == \"concat\":\n",
        "        attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "      elif self.method == \"dot\":\n",
        "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "      attn_energies = attn_energies.t()\n",
        "\n",
        "      return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "0tyfRsV9_K8D"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "  def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "    super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "    self.attn_model = attn_model\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.embedding = embedding\n",
        "    self.embedding_dropout = nn.Dropout(dropout)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "    self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "  def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "    embedded = self.embedding(input_step)\n",
        "    embedded = self.embedding_dropout(embedded)\n",
        "    rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "    attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "    context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "    rnn_output = rnn_output.squeeze(0)\n",
        "    context = context.squeeze(1)\n",
        "    concat_input = torch.cat((rnn_output, context), 1)\n",
        "    concat_output = torch.tanh(self.concat(concat_input))\n",
        "    output = self.out(concat_output)\n",
        "    output = F.softmax(output, dim=1)\n",
        "    return output, hidden"
      ],
      "metadata": {
        "id": "rweHH3-qAfX8"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "  nTotal = mask.sum()\n",
        "  crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "  loss = crossEntropy.masked_select(mask).mean()\n",
        "  loss = loss.to(device)\n",
        "  return loss, nTotal.item()"
      ],
      "metadata": {
        "id": "-eAxrkO4BALn"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encider,\n",
        "          decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size,\n",
        "          clip, max_length=MAX_LENGTH):\n",
        "  encoder_optimizer.zero_grad()\n",
        "  decoder_optimizer.zero_grad()\n",
        "  input_variable = input_variable.to(device)\n",
        "  target_variable = target_variable.to(device)\n",
        "  mask = mask.to(device)\n",
        "  lengths = lengths.to(\"cpu\")\n",
        "\n",
        "  loss = 0\n",
        "  print_losses = []\n",
        "  n_totals = 0\n",
        "\n",
        "  encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "  decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "  decoder_input = decoder_input.to(device)\n",
        "\n",
        "  decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "  use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "  if use_teacher_forcing:\n",
        "    for t in range(max_target_len):\n",
        "      decoder_output, decoder_hidden = decoder(\n",
        "          decoder_input, decoder_hidden, encoder_outputs\n",
        "      )\n",
        "      decoder_input = target_variable[t].view(1, -1)\n",
        "      mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "      loss += mask_loss\n",
        "      print_losses.append(mask_loss.item() * nTotal)\n",
        "      n_totals += nTotal\n",
        "    else:\n",
        "      for t in range(max_target_len):\n",
        "        decoder_output, decoder_hidden = decoder(\n",
        "            decoder_input, decoder_hidden, encoder_outputs\n",
        "        )\n",
        "        _, topi = decoder_output.topk(1)\n",
        "        decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "        decoder_input = decoder_input.to(device)\n",
        "        mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "        loss += mask_loss\n",
        "        print_losses.append(mask_loss.item() * nTotal)\n",
        "        n_totals += nTotal\n",
        "        print_losses.append(mask_loss.item() * nTotal)\n",
        "        n_totals += nTotal\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ],
      "metadata": {
        "id": "wX4bS0IpOe8A"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "               embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration,\n",
        "               batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
        "  training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "  print(\"Start training!\")\n",
        "  start_iteration = 1\n",
        "  print_loss = 0\n",
        "  if loadFilename:\n",
        "    start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "  print(\"Training...\")\n",
        "  for iteration in range(start_iteration, n_iteration + 1):\n",
        "    training_batch = training_batches[iteration - 1]\n",
        "    input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "    loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                 decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "    print_loss += loss\n",
        "    if iteration % print_every == 0:\n",
        "      print_loss_avg = print_loss / print_every\n",
        "      print(\"Iteration: {}; Percent complete: {:.1f}%; Average Loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "      print_loss = 0\n",
        "\n",
        "    if (iteration % save_every == 0):\n",
        "      directory = os.path.join(save_dir, model_name, corpus_name,\n",
        "                               '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "      if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "      torch.save({\n",
        "          'iteration': iteration,\n",
        "          'en': encoder.state_dict(),\n",
        "          'de': decoder.state_dict(),\n",
        "          'en_opt': encoder_optimizer.state_dict(),\n",
        "          'de_opt': decoder_optimizer.state_dict(),\n",
        "          'loss': loss,\n",
        "          'voc_dict': voc.__dict__,\n",
        "          'embedding': embedding.state_dict()\n",
        "      }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "ZymdowvcPs4U"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "  def __init__(self, encoder, decoder):\n",
        "    super(GreedySearchDecoder, self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "\n",
        "  def forward(self, input_seq, input_length, max_length):\n",
        "    encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "    decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
        "    all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "    all_scores = torch.zeros([0], device=device)\n",
        "    for _ in range(max_length):\n",
        "      decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "      decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "      all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "      all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "      decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "    return all_tokens, all_scores\n",
        ""
      ],
      "metadata": {
        "id": "YuJUkvPmQ6zg"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "  indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "  lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "  input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "  input_batch = input_batch.to(device)\n",
        "  lengths = lengths.to(\"cpu\")\n",
        "  tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "  decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "  return decoded_words\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "  input_sentence = ''\n",
        "  while(1):\n",
        "    try:\n",
        "      input_sentence = input('> ')\n",
        "      if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "      input_sentence = normalizeString(input_sentence)\n",
        "      output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "      output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "      print('Bot:', ' '.join(output_words))\n",
        "    except KeyError:\n",
        "      print(\"Error: Encountered unknown word.\")\n"
      ],
      "metadata": {
        "id": "F3A_dkzGRpmx"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"cb_model\"\n",
        "attn_model = \"dot\"\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "loadFilename = None\n",
        "checkpoint_iter = 4000"
      ],
      "metadata": {
        "id": "ecRkIwKGTPS1"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "                           f'{encoder_n_layers}-{decoder_n_layers}_{hidden_size}',\n",
        "                           f'{checkpoint_iter}_checkpoint.tar')\n",
        "\n",
        "print(f\"Attempting to load from: {loadFilename}\")\n",
        "\n",
        "# Verify the file exists\n",
        "if not os.path.exists(loadFilename):\n",
        "    print(f\"Error: File not found at {loadFilename}\")\n",
        "    print(\"Please check if the file exists in your Google Drive at this location\")\n",
        "else:\n",
        "    print(\"File found!\")\n",
        "\n",
        "if loadFilename:\n",
        "    checkpoint = torch.load(loadFilename, weights_only=True, map_location=device)\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "print(\"Building Encoder and Decoder...\")\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size,\n",
        "                             voc.num_words, decoder_n_layers, dropout)\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print(\"Models built and ready to go!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "PqGjwWE7Ug42",
        "outputId": "38c3025f-dbb8-4e36-f6e5-809c59da6f17"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load from: /content/drive/MyDrive/chatbot_model/2-2_500/4000_checkpoint.tar\n",
            "Error: File not found at /content/drive/MyDrive/chatbot_model/2-2_500/4000_checkpoint.tar\n",
            "Please check if the file exists in your Google Drive at this location\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/chatbot_model/2-2_500/4000_checkpoint.tar'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-518f38d93c4a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mloadFilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadFilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mencoder_sd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdecoder_sd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'de'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/chatbot_model/2-2_500/4000_checkpoint.tar'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 4000\n",
        "print_every = 1\n",
        "save_every = 500\n",
        "\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "  encoder_optimizer.load_state_dict(checkpoint['en_opt'])\n",
        "  decoder_optimizer.load_state_dict(checkpoint['de_opt'])\n",
        "\n",
        "for state in encoder_optimizer.state.values():\n",
        "  for k, v in state.items():\n",
        "    if isinstance(v, torch.Tensor):\n",
        "      state[k] = v.to(device)\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "  for k, v in state.items():\n",
        "    if isinstance(v, torch.Tensor):\n",
        "      state[k] = v.to(device)\n",
        "\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)\n"
      ],
      "metadata": {
        "id": "6Mnfs_6FTiEO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}